#Multiclass Classification logistic regression
%matplotlib inline
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
digits = load_digits()
dir(digits)
digits.data[0]
plt.gray()
plt.matshow(digits.images[0])
plt.imshow(digits.images[0])
plt.show()
plt.gray()
for i in range(5):
  plt.matshow(digits.images[i])
  plt.show()
  digits.target[0:5]
  from sklearn.model_selection import train_test_split
  X_train, X_test, y_train, y_test = train_test_split(digits.data,digits.target, test_size=0.2)
  len(X_train)
  len(X_test)
  from sklearn.linear_model import LogisticRegression
  model = LogisticRegression()
  model.fit(X_train, y_train)
  model.score(X_test, y_test)
  model.predict(X_test[0:5])
  import matplotlib.pyplot as plt
  plt.matshow(digits.images[0])
  plt.show()
  digits.target[6]
  y_predicted = model.predict(X_test)
  from sklearn.metrics import confusion_matrix
  cm = confusion_matrix(y_test, y_predicted)
  #seaborn for visualization
  import seaborn as sn
  plt.figure(figsize = (10,7))
  sn.heatmap(cm, annot=True)


# Use sklearn.datasets iris flower dataset to train your model using logistic regression. You need to figure out accuracy of your model and use that to predict different samples in your test dataset. In iris dataset there are 150 samples containing following features,
# Sepal Length
# Sepal Width
# Petal Length
# Petal Width
# Using above 4 features you will clasify a flower in one of the three categories,
# Setosa
# Versicolour
# Virginica

from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


iris = load_iris()
X, y = iris.data, iris.target


X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")

for i in range(5):
    sample = X_test[i].reshape(1, -1)
    prediction = model.predict(sample)
    print(f"Sample {i+1}: Features={X_test[i]}, Predicted={iris.target_names[prediction][0]}, Actual={iris.target_names[y_test[i]]}")


#Decision Tree
import pandas as pd
df = pd.read_csv('/content/salaries.csv')
df
inputs = df.drop('salary_more_then_100k',axis='columns')
target = df['salary_more_then_100k']
inputs
target
from sklearn.preprocessing import LabelEncoder
le_company = LabelEncoder()
le_job = LabelEncoder()
le_degree = LabelEncoder()
inputs['company_n'] = le_company.fit_transform(inputs['company'])
inputs['job_n'] = le_job.fit_transform(inputs['job'])
inputs['degree_n'] = le_degree.fit_transform(inputs['degree'])
inputs
inputs_n = inputs.drop(['company','job','degree'],axis='columns')
inputs_n
from sklearn import tree
model = tree.DecisionTreeClassifier()
model.fit(inputs_n, target)
model.score(inputs_n,target)
model.predict([[2,1,0]])
model.predict([[2,1,1]])


import pandas as pd
df = pd.read_csv('/content/titanic.csv')
df.head()
df.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked'],axis='columns',inplace=True)
df.head
inputs = df.drop('Survived',axis='columns')
target = df.Survived
inputs.Sex = inputs.Sex.map({'male': 1, 'female': 2})

inputs.Age = inputs.Age.fillna(inputs.Age.mean())
inputs.head()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(inputs,target,test_size=0.2)
len(X_train)
from sklearn import tree
model = tree.DecisionTreeClassifier()
model.fit(X_train,y_train)
model.score(X_test,y_test)
model.predict([[3,1,22.0,7.2500]])

# Support Vector Machine using Sklearn (SVM)
import pandas as pd
from sklearn.datasets import load_iris
iris = load_iris()
dir(iris)
iris.feature_names
df = pd.DataFrame(iris.data,columns=iris.feature_names)
df.head()
df['target'] = iris.target
df.head()
iris.target_names
df[df.target==1].head()
iris.target_names
df['flower_name'] =df.target.apply(lambda x: iris.target_names[x])
df.head()
from matplotlib import pyplot as plt
%matplotlib inline
df0 = df[df.target==0]
df1 = df[df.target==1]
df2 = df[df.target==2]
df2.head()
plt.xlabel('sepal length (cm)')
plt.ylabel('sepal width (cm)')
plt.scatter(df0['sepal length (cm)'], df0['sepal width (cm)'],color="red",marker='+')
plt.scatter(df1['sepal length (cm)'], df1['sepal width (cm)'],color="blue",marker='.')
from sklearn.model_selection import train_test_split
X = df.drop(['target','flower_name'], axis='columns')
y = df.target
X
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
len(X_train)
from sklearn.svm import SVC #(SVC classifier)
model = SVC()
model = SVC(kernel='linear')
model.fit(X_train, y_train)
model.score(X_test, y_test)

#Random Forest Algorithm (RFA)
import pandas as pd
from sklearn.datasets import load_digits
digits = load_digits()
dir(digits)

%matplotlib inline
import matplotlib.pyplot as plt
plt.gray()
for i in range(4):
  plt.matshow(digits.images[i])
  plt.show()

digits.data[:5]
df = pd.DataFrame(digits.data)
df.head()
df['target']=digits.target
df.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop(['target'],axis='columns'),digits.target,test_size=0.2)
len(X_train)
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=20)
model.fit(X_train, y_train)
model.score(X_test, y_test)

y_predicted = model.predict(X_test)
from sklearn.metrics import confusion_matrix
cm = confusion_matrix (y_test,y_predicted)
cm
%matplotlib inline
import seaborn as sn
plt.figure(figsize = (10,7))
sn.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

from sklearn.datasets import load_iris
iris = load_iris()
dir(iris)
import pandas as pd
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df.head()
df['target'] = iris.target
df.head()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop(['target'],axis='columns'),iris.target,test_size=0.2)
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=20)
model.fit(X_train, y_train)
model.score(X_test, y_test)
model = RandomForestClassifier(n_estimators=40)
model.fit(X_train, y_train)
model.score(X_test, y_test)

# K Fold Cross Validation
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
import numpy as np
from sklearn.datasets import load_digits

digits = load_digits()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(digits.data,digits.target,test_size=0.3)
#Logistic Regression using Classifier
lr = LogisticRegression(solver='liblinear',multi_class='ovr')
lr.fit(X_train, y_train)
lr.score(X_test, y_test)

# svm = SVC(gamma='auto')
# svm.fit(X_train, y_train)
# svm.score(X_test, y_test)

rf = RandomForestClassifier(n_estimators=40)
rf.fit(X_train, y_train)
rf.score(X_test, y_test)

# kfold specifying
from sklearn.model_selection import KFold
kf = KFold(n_splits=3)
kf
for train_index, test_index in kf.split([1,2,3,4,5,6,7,8,9]):
  print(train_index, test_index)
  #Measuring
def get_score(model, X_train, X_test, y_train, y_test):
  model.fit(X_train, y_train)
  return model.score(X_test, y_test)

  print(get_score(LogisticRegression(solver='liblinear',multi_class='ovr'), X_train, X_test, y_train, y_test))
  print(get_score(SVC(gamma='auto'), X_train, X_test, y_train, y_test))
  print(get_score(RandomForestClassifier(n_estimators=40), X_train, X_test, y_train, y_test))

from sklearn.model_selection import StratifiedKFold
folds = StratifiedKFold(n_splits=3)
# model scores
scores_logistic = []
scores_svm = []
scores_rf = []
for train_index, test_index in folds.split(digits.data,digits.target):
  X_train, X_test, y_train, y_test = digits.data[train_index], digits.data[test_index], \
                                          digits.target[train_index], digits.target[test_index]
  scores_logistic.append(get_score(LogisticRegression(solver='liblinear',multi_class='ovr'), X_train, X_test, y_train, y_test))
  scores_svm.append(get_score(SVC(gamma='auto'), X_train, X_test, y_train, y_test))
  scores_rf.append(get_score(RandomForestClassifier(n_estimators=40), X_train, X_test, y_train, y_test))

print(scores_logistic)
print(scores_svm)
print(scores_rf)

from sklearn.model_selection import cross_val_score
cross_val_score(LogisticRegression(solver='liblinear',multi_class='ovr'), digits.data, digits.target,cv=3)
cross_val_score(SVC(gamma='auto'), digits.data, digits.target,cv=3)
cross_val_score(RandomForestClassifier(n_estimators=40),digits.data, digits.target,cv=3)
cross_val_score(RandomForestClassifier(n_estimators=5),digits.data, digits.target,cv=3)


#K means clustrring algorithm
from sklearn.cluster import KMeans
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from matplotlib import pyplot as plt
%matplotlib inline
df = pd.read_csv("/content/income.csv")
df
plt.scatter(df.Age,df['Income($)'])
plt.xlabel('Age')
plt.ylabel('Income($)')
km = KMeans(n_clusters=3, n_init=10, random_state=0)
y_predicted = km.fit_predict(df[['Age','Income($)']].values)
y_predicted
df['cluster']=y_predicted
df.head()
plt.scatter(df.Age,df['Income($)'],c=y_predicted,cmap='rainbow')
df

scaler = MinMaxScaler()
scaler.fit(df[['Income($)']])
df['Income($)'] = scaler.transform(df[['Income($)']])
df

scaler.fit(df[['Age']])
df.Age = scaler.transform(df[['Age']])
df

km = KMeans(n_clusters=3, n_init=10, random_state=0)
y_predicted = km.fit_predict(df[['Age','Income($)']].values)
y_predicted
df['cluster']=y_predicted
df
df.drop(['cluster'],axis='columns',inplace=True)
df
plt.scatter(df.Age,df['Income($)'],c=y_predicted,cmap='rainbow')
plt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],color='black')

k_rng = range(1,10)
sse = []
for k in k_rng:
  km = KMeans(n_clusters=k, n_init=10, random_state=0)
  km.fit(df[['Age','Income($)']].values)
  sse.append(km.inertia_)
plt.xlabel('K')
plt.ylabel('Sum of squared error')
plt.plot(k_rng,sse)


# #Naive Bayes Classifier

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
df = pd.read_csv("/content/titanic.csv")
target = df.Survived
df.drop(['PassengerId','Name','Survived','SibSp','Parch','Ticket','Cabin','Embarked'], axis='columns', inplace=True)
dummies = pd.get_dummies(df.Sex, drop_first=False).astype(int)
inputs = pd.concat([df.drop('Sex', axis='columns'), dummies], axis='columns')
inputs.Age = inputs.Age.fillna(inputs.Age.mean())
X_train, X_test, y_train, y_test = train_test_split(inputs, target, test_size=0.3)
model = GaussianNB()
model.fit(X_train, y_train)
print("Model Accuracy:", model.score(X_test, y_test))
X_test[:10]
y_test[:10]
model.predict(X_test[:10])
model.predict_proba(X_test[:10])

# Naive Bayes Classifier
import pandas as pd
df = pd.read_csv("/content/spam.csv")
df.head()
df.groupby('Category').describe()
df['spam'] = df['Category'].apply(lambda x: 1 if x == 'spam' else 0)
df.head()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.Message, df.spam)
from sklearn.feature_extraction.text import CountVectorizer
v = CountVectorizer()
X_train_count = v.fit_transform(X_train.values)
emails = {
    'Hey mohan, can we get together to watch footbal game tomorrow?',
    'Upto 20% discount on parking, exclusive offer just for you. Dont miss this reward!'
}
emails_count = v.transform(emails)
X_test_count = v.transform(X_test)

from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()
model.fit(X_train_count, y_train)
emails_count = v.transform(emails)
emails_count.toarray()
from sklearn.pipeline import Pipeline
clf = Pipeline([
    ('vectorizer', CountVectorizer()),
    ('nb', MultinomialNB())
])
clf.fit(X_train, y_train)
clf.score(X_test, y_test)
clf.predict(emails)

# Hyper parameter tuning
import matplotlib.pyplot as plt
import numpy as np
from sklearn import svm, datasets
import pandas as pd
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression


iris = datasets.load_iris()

df = pd.DataFrame(iris.data,columns=iris.feature_names)
df['flower'] = iris.target
df['flower'] = df['flower'].apply(lambda x: iris.target_names[x])
df[47:52]
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3)
model = svm.SVC(kernel='rbf',C=30,gamma='auto')
model.fit(X_train,y_train)
model.score(X_test, y_test)
from sklearn.model_selection import cross_val_score
cross_val_score(svm.SVC(kernel='linear',C=10,gamma='auto'),iris.data, iris.target, cv=5)
kernels = ['rbf', 'linear']
C = [1,10,20]
avg_scores = {}
#using kenrel for similarity
# for kval in kernels:
#   for cval in C:
#     cv_scores = cross_val_score(svm.SVC(kernel=kval,C=cval,gamma='auto'),iris.data, iris.target, cv=5)
#     avg_scores[kval + '_' + str(cval)] = np.average(cv_scores)
# avg_scores

#using grid search for accessibiltiy
from sklearn.model_selection import GridSearchCV
clf = GridSearchCV(svm.SVC(gamma='auto'), {
    'C': [1,10,20],
    'kernel': ['rbf','linear']
}, cv=5, return_train_score=False)
clf.fit(iris.data, iris.target)
clf.cv_results_
df = pd.DataFrame(clf.cv_results_)
df
df = pd.DataFrame(clf.cv_results_)
df[['param_C','param_kernel','mean_test_score']]
dir(clf)
clf.best_score_
clf.best_params_
from sklearn.model_selection import RandomizedSearchCV
rs = RandomizedSearchCV(svm.SVC(gamma='auto'), {
        'C': [1,10,20],
        'kernel': ['rbf','linear']
    },
    cv=5, n_iter=5)
rs.fit(iris.data, iris.target)
pd.DataFrame(rs.cv_results_)[['param_C','param_kernel','mean_test_score']]

#Choosing a model
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

models = {
    'svm': {
        'model': svm.SVC(gamma='auto'),
        'params' : {
            'C': [1,10,20],
            'kernel': ['rbf','linear']
        }
    },
    'random_forest': {
        'model': RandomForestClassifier(),
        'params' : {
            'n_estimators': [1,5,10]
        }
    },
    'logistic_regression' : {
        'model': LogisticRegression(solver='liblinear',multi_class='auto'),
        'params': {
            'C': [1,5,10]
        }
    }
}

scores = []
for model_name, mp in models.items():
  clf = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)
  clf.fit(iris.data, iris.target)
  scores.append({
      'model': model_name,
      'best_score': clf.best_score_,
      'best_params': clf.best_params_
  })
  df = pd.DataFrame(scores,columns=['model','best_score','best_params'])
  df
  plt.figure(figsize=(8,5))
plt.bar(df['model'], df['best_score'], color='skyblue')
plt.title("Model Comparison - Best Cross Validation Score")
plt.xlabel("Model")
plt.ylabel("Best CV Accuracy")
plt.ylim(0.9, 1.0)
for i, v in enumerate(df['best_score']):
    plt.text(i, v + 0.005, f"{v:.3f}", ha='center')
plt.show()
df


# L1 and L2 Regularization (underfit, overfit, balanced fit )
# import numpy as np
# import matplotlib.pyplot as plt
# import pandas as pd
# import seaborn as sns
# import warnings
# warnings.filterwarnings('ignore')
# dataset = pd.read_csv('/content/Melbourne_housing_FULL.csv')
# dataset.head()
# dataset.nunique()
# dataset.shape
# dataset.isna().sum()
# cols_to_fill_zero = ['Propertycount', 'Distance', 'Bedroom2', 'Bathroom', 'Car']
# dataset[cols_to_fill_zero] = dataset[cols_to_fill_zero].fillna(0)
# dataset.isna().sum()
# dataset['Landsize'] = dataset['Landsize'].fillna(dataset.Landsize.mean())
# dataset['BuildingArea'] = dataset['BuildingArea'].fillna(dataset.BuildingArea.mean())
# dataset.isna().sum()
# dataset.dropna(inplace=True)
# dataset.isna().sum()
# X = dataset.drop(['Price', 'Date'], axis=1)
# y = dataset['Price']

# from sklearn.model_selection import train_test_split
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)
# from sklearn.linear_model import LinearRegression
# reg = LinearRegression().fit(X_train, y_train)
# reg.score(X_test, y_test)
# reg.score(X_train, y_train)
# from sklearn import linear_model
# lasso_reg = linear_model.Lasso(alpha=50, max_iter=100, tol=0.1)
# lasso_reg.fit(X_train, y_train)
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Load dataset
dataset = pd.read_csv('/content/Melbourne_housing_FULL.csv')

# Quick checks
print(dataset.head())
print(dataset.nunique())
print(dataset.shape)
print(dataset.isna().sum())

# Fill missing values
cols_to_fill_zero = ['Propertycount', 'Distance', 'Bedroom2', 'Bathroom', 'Car']
dataset[cols_to_fill_zero] = dataset[cols_to_fill_zero].fillna(0)

dataset['Landsize'] = dataset['Landsize'].fillna(dataset.Landsize.mean())
dataset['BuildingArea'] = dataset['BuildingArea'].fillna(dataset.BuildingArea.mean())

# Drop rows with any remaining NaN
dataset.dropna(inplace=True)

print(dataset.isna().sum())

#  Drop all string/categorical columns 
X = dataset.drop([
    'Price', 'Date', 'Suburb', 'Address', 'SellerG',
    'CouncilArea', 'Regionname', 'Type', 'Method'
], axis=1)
y = dataset['Price']

# Split data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)

# Linear Regression
from sklearn.linear_model import LinearRegression
reg = LinearRegression().fit(X_train, y_train)

print("Linear Regression Train Score:", reg.score(X_train, y_train))
print("Linear Regression Test Score:", reg.score(X_test, y_test))

# Lasso Regression
from sklearn import linear_model
lasso_reg = linear_model.Lasso(alpha=50, max_iter=100, tol=0.1)
lasso_reg.fit(X_train, y_train)

print("Lasso Regression Train Score:", lasso_reg.score(X_train, y_train))
print("Lasso Regression Test Score:", lasso_reg.score(X_test, y_test))

from sklearn.linear_model import Ridge
ridge_reg = Ridge(alpha=50, max_iter=100, tol=0.1)
ridge_reg.fit(X_train, y_train)
ridge_reg.score(X_train, y_train)
ridge_reg.score(X_test, y_test)



#K nearest neighbors classification
import pandas as pd
from sklearn.datasets import load_iris
iris = load_iris()
iris.feature_names
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df.head()
df.shape
df['target'] = iris.target
df.head()
df['flower_name'] = df.target.apply(lambda x: iris.target_names[x])
df[df.target==1].head()
df[df.target==2].head()
df.shape
df['target'] = iris.target
df.head()
import matplotlib.pyplot as plt
%matplotlib inline
df0 = df[df.target==0]
df1 = df[df.target==1]
df2 = df[df.target==2]
plt.xlabel('sepal length (cm)')
plt.ylabel('sepal width (cm)')
plt.scatter(df0['sepal length (cm)'], df0['sepal width (cm)'], color='green', marker='+')
plt.scatter(df1['sepal length (cm)'], df1['sepal width (cm)'], color='blue', marker='.')
from sklearn.model_selection import train_test_split
X = df.drop(['target', 'flower_name'], axis='columns')
y = df.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state =1)
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=10)
knn.fit(X_train, y_train)
knn.score(X_test, y_test)
y_pred = knn.predict(X_test)
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
cm
import seaborn as sn
plt.figure(figsize=(10,7))
sn.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))
























