#Multiclass Classification logistic regression
%matplotlib inline
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
digits = load_digits()
dir(digits)
digits.data[0]
plt.gray()
plt.matshow(digits.images[0])
plt.imshow(digits.images[0])
plt.show()
plt.gray()
for i in range(5):
  plt.matshow(digits.images[i])
  plt.show()
  digits.target[0:5]
  from sklearn.model_selection import train_test_split
  X_train, X_test, y_train, y_test = train_test_split(digits.data,digits.target, test_size=0.2)
  len(X_train)
  len(X_test)
  from sklearn.linear_model import LogisticRegression
  model = LogisticRegression()
  model.fit(X_train, y_train)
  model.score(X_test, y_test)
  model.predict(X_test[0:5])
  import matplotlib.pyplot as plt
  plt.matshow(digits.images[0])
  plt.show()
  digits.target[6]
  y_predicted = model.predict(X_test)
  from sklearn.metrics import confusion_matrix
  cm = confusion_matrix(y_test, y_predicted)
  #seaborn for visualization
  import seaborn as sn
  plt.figure(figsize = (10,7))
  sn.heatmap(cm, annot=True)


# Use sklearn.datasets iris flower dataset to train your model using logistic regression. You need to figure out accuracy of your model and use that to predict different samples in your test dataset. In iris dataset there are 150 samples containing following features,
# Sepal Length
# Sepal Width
# Petal Length
# Petal Width
# Using above 4 features you will clasify a flower in one of the three categories,
# Setosa
# Versicolour
# Virginica

from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


iris = load_iris()
X, y = iris.data, iris.target


X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")

for i in range(5):
    sample = X_test[i].reshape(1, -1)
    prediction = model.predict(sample)
    print(f"Sample {i+1}: Features={X_test[i]}, Predicted={iris.target_names[prediction][0]}, Actual={iris.target_names[y_test[i]]}")


#Decision Tree
import pandas as pd
df = pd.read_csv('/content/salaries.csv')
df
inputs = df.drop('salary_more_then_100k',axis='columns')
target = df['salary_more_then_100k']
inputs
target
from sklearn.preprocessing import LabelEncoder
le_company = LabelEncoder()
le_job = LabelEncoder()
le_degree = LabelEncoder()
inputs['company_n'] = le_company.fit_transform(inputs['company'])
inputs['job_n'] = le_job.fit_transform(inputs['job'])
inputs['degree_n'] = le_degree.fit_transform(inputs['degree'])
inputs
inputs_n = inputs.drop(['company','job','degree'],axis='columns')
inputs_n
from sklearn import tree
model = tree.DecisionTreeClassifier()
model.fit(inputs_n, target)
model.score(inputs_n,target)
model.predict([[2,1,0]])
model.predict([[2,1,1]])


import pandas as pd
df = pd.read_csv('/content/titanic.csv')
df.head()
df.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked'],axis='columns',inplace=True)
df.head
inputs = df.drop('Survived',axis='columns')
target = df.Survived
inputs.Sex = inputs.Sex.map({'male': 1, 'female': 2})

inputs.Age = inputs.Age.fillna(inputs.Age.mean())
inputs.head()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(inputs,target,test_size=0.2)
len(X_train)
from sklearn import tree
model = tree.DecisionTreeClassifier()
model.fit(X_train,y_train)
model.score(X_test,y_test)
model.predict([[3,1,22.0,7.2500]])

# Support Vector Machine using Sklearn (SVM)
import pandas as pd
from sklearn.datasets import load_iris
iris = load_iris()
dir(iris)
iris.feature_names
df = pd.DataFrame(iris.data,columns=iris.feature_names)
df.head()
df['target'] = iris.target
df.head()
iris.target_names
df[df.target==1].head()
iris.target_names
df['flower_name'] =df.target.apply(lambda x: iris.target_names[x])
df.head()
from matplotlib import pyplot as plt
%matplotlib inline
df0 = df[df.target==0]
df1 = df[df.target==1]
df2 = df[df.target==2]
df2.head()
plt.xlabel('sepal length (cm)')
plt.ylabel('sepal width (cm)')
plt.scatter(df0['sepal length (cm)'], df0['sepal width (cm)'],color="red",marker='+')
plt.scatter(df1['sepal length (cm)'], df1['sepal width (cm)'],color="blue",marker='.')
from sklearn.model_selection import train_test_split
X = df.drop(['target','flower_name'], axis='columns')
y = df.target
X
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
len(X_train)
from sklearn.svm import SVC #(SVC classifier)
model = SVC()
model = SVC(kernel='linear')
model.fit(X_train, y_train)
model.score(X_test, y_test)

#Random Forest Algorithm (RFA)
import pandas as pd
from sklearn.datasets import load_digits
digits = load_digits()
dir(digits)

%matplotlib inline
import matplotlib.pyplot as plt
plt.gray()
for i in range(4):
  plt.matshow(digits.images[i])
  plt.show()

digits.data[:5]
df = pd.DataFrame(digits.data)
df.head()
df['target']=digits.target
df.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop(['target'],axis='columns'),digits.target,test_size=0.2)
len(X_train)
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=20)
model.fit(X_train, y_train)
model.score(X_test, y_test)

y_predicted = model.predict(X_test)
from sklearn.metrics import confusion_matrix
cm = confusion_matrix (y_test,y_predicted)
cm
%matplotlib inline
import seaborn as sn
plt.figure(figsize = (10,7))
sn.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

from sklearn.datasets import load_iris
iris = load_iris()
dir(iris)
import pandas as pd
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df.head()
df['target'] = iris.target
df.head()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop(['target'],axis='columns'),iris.target,test_size=0.2)
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=20)
model.fit(X_train, y_train)
model.score(X_test, y_test)
model = RandomForestClassifier(n_estimators=40)
model.fit(X_train, y_train)
model.score(X_test, y_test)

# K Fold Cross Validation
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
import numpy as np
from sklearn.datasets import load_digits

digits = load_digits()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(digits.data,digits.target,test_size=0.3)
#Logistic Regression using Classifier
lr = LogisticRegression(solver='liblinear',multi_class='ovr')
lr.fit(X_train, y_train)
lr.score(X_test, y_test)

# svm = SVC(gamma='auto')
# svm.fit(X_train, y_train)
# svm.score(X_test, y_test)

rf = RandomForestClassifier(n_estimators=40)
rf.fit(X_train, y_train)
rf.score(X_test, y_test)

# kfold specifying
from sklearn.model_selection import KFold
kf = KFold(n_splits=3)
kf
for train_index, test_index in kf.split([1,2,3,4,5,6,7,8,9]):
  print(train_index, test_index)
  #Measuring
def get_score(model, X_train, X_test, y_train, y_test):
  model.fit(X_train, y_train)
  return model.score(X_test, y_test)

  print(get_score(LogisticRegression(solver='liblinear',multi_class='ovr'), X_train, X_test, y_train, y_test))
  print(get_score(SVC(gamma='auto'), X_train, X_test, y_train, y_test))
  print(get_score(RandomForestClassifier(n_estimators=40), X_train, X_test, y_train, y_test))

from sklearn.model_selection import StratifiedKFold
folds = StratifiedKFold(n_splits=3)
# model scores
scores_logistic = []
scores_svm = []
scores_rf = []
for train_index, test_index in folds.split(digits.data,digits.target):
  X_train, X_test, y_train, y_test = digits.data[train_index], digits.data[test_index], \
                                          digits.target[train_index], digits.target[test_index]
  scores_logistic.append(get_score(LogisticRegression(solver='liblinear',multi_class='ovr'), X_train, X_test, y_train, y_test))
  scores_svm.append(get_score(SVC(gamma='auto'), X_train, X_test, y_train, y_test))
  scores_rf.append(get_score(RandomForestClassifier(n_estimators=40), X_train, X_test, y_train, y_test))

print(scores_logistic)
print(scores_svm)
print(scores_rf)

from sklearn.model_selection import cross_val_score
cross_val_score(LogisticRegression(solver='liblinear',multi_class='ovr'), digits.data, digits.target,cv=3)
cross_val_score(SVC(gamma='auto'), digits.data, digits.target,cv=3)
cross_val_score(RandomForestClassifier(n_estimators=40),digits.data, digits.target,cv=3)
cross_val_score(RandomForestClassifier(n_estimators=5),digits.data, digits.target,cv=3)


#K means clustrring algorithm
from sklearn.cluster import KMeans
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from matplotlib import pyplot as plt
%matplotlib inline
df = pd.read_csv("/content/income.csv")
df
plt.scatter(df.Age,df['Income($)'])
plt.xlabel('Age')
plt.ylabel('Income($)')
km = KMeans(n_clusters=3, n_init=10, random_state=0)
y_predicted = km.fit_predict(df[['Age','Income($)']].values)
y_predicted
df['cluster']=y_predicted
df.head()
plt.scatter(df.Age,df['Income($)'],c=y_predicted,cmap='rainbow')
df

scaler = MinMaxScaler()
scaler.fit(df[['Income($)']])
df['Income($)'] = scaler.transform(df[['Income($)']])
df

scaler.fit(df[['Age']])
df.Age = scaler.transform(df[['Age']])
df

km = KMeans(n_clusters=3, n_init=10, random_state=0)
y_predicted = km.fit_predict(df[['Age','Income($)']].values)
y_predicted
df['cluster']=y_predicted
df
df.drop(['cluster'],axis='columns',inplace=True)
df
plt.scatter(df.Age,df['Income($)'],c=y_predicted,cmap='rainbow')
plt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],color='black')

k_rng = range(1,10)
sse = []
for k in k_rng:
  km = KMeans(n_clusters=k, n_init=10, random_state=0)
  km.fit(df[['Age','Income($)']].values)
  sse.append(km.inertia_)
plt.xlabel('K')
plt.ylabel('Sum of squared error')
plt.plot(k_rng,sse)


# #Naive Bayes Classifier

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
df = pd.read_csv("/content/titanic.csv")
target = df.Survived
df.drop(['PassengerId','Name','Survived','SibSp','Parch','Ticket','Cabin','Embarked'], axis='columns', inplace=True)
dummies = pd.get_dummies(df.Sex, drop_first=False).astype(int)
inputs = pd.concat([df.drop('Sex', axis='columns'), dummies], axis='columns')
inputs.Age = inputs.Age.fillna(inputs.Age.mean())
X_train, X_test, y_train, y_test = train_test_split(inputs, target, test_size=0.3)
model = GaussianNB()
model.fit(X_train, y_train)
print("Model Accuracy:", model.score(X_test, y_test))
X_test[:10]
y_test[:10]
model.predict(X_test[:10])
model.predict_proba(X_test[:10])































